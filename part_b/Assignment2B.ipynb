{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing the required libraries.\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers,models\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense, Activation, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "-kHt9c5gpRJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://keras.io/api/applications/\n",
        "\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.applications.xception import Xception\n",
        "# from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.mobilenet_v2 import MobileNetV2"
      ],
      "metadata": {
        "id": "SxYYhRDI-W6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying the tensorflow version is latest 2.8.0\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "rtc4dqs3px2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the inaturalist dataset.\n",
        "dataset_url = \"https://storage.googleapis.com/wandb_datasets/nature_12K.zip\"\n",
        "dataset_dir = tf.keras.utils.get_file(\"nature_12K\",origin=dataset_url,cache_dir='.',extract=True)\n"
      ],
      "metadata": {
        "id": "N-Yklrb0rBgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the train dataset and test dataset directory.\n",
        "trainset_dir = './datasets/inaturalist_12K/train/'\n",
        "testset_dir = './datasets/inaturalist_12K/val/'\n",
        "classlist = [name for name in os.listdir(trainset_dir) if os.path.isdir(os.path.join(trainset_dir, name))]\n",
        "# classlist"
      ],
      "metadata": {
        "id": "WprU_0WnyLfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the wandb for experimental tracking and reporting.\n",
        "# !pip install wandb --upgrade     #For experiment tracking\n",
        "# import wandb\n",
        "# wandb.login()"
      ],
      "metadata": {
        "id": "WH2vnsxqQRAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the train dataset and val dataset.\n",
        "def generate_batch_train_val(path, augmentation, batch_size, image_size):\n",
        "    rescaledata = tf.keras.layers.Rescaling(1./127.5, offset=-1)\n",
        "    #Splits the dataset into train and validation.\n",
        "    #Keras' ImageDataGenerator is used to split data into train and test. \n",
        "    if augmentation:\n",
        "        #Applies data augmentation if specified\n",
        "        train_data_gen = ImageDataGenerator(\n",
        "                            rescale = 1./255,\n",
        "                            horizontal_flip = True,\n",
        "                            rotation_range = 30,\n",
        "                            shear_range = 0.2,\n",
        "                            zoom_range = 0.2,\n",
        "                            width_shift_range = 0.2,\n",
        "                            height_shift_range = 0.2,\n",
        "                            validation_split = 0.1,\n",
        "                        )\n",
        "    else:\n",
        "        train_data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.10)\n",
        "\n",
        "    #Flow from directory expects that images belonging to each class is present in its own folder but inside the same parent folder : data directory.\n",
        "    #It takes path to the data directory as input and generates batches of desired batch size.\n",
        "    #Need to specify appropriate subset (training / validation) to generate batches for respective subset.\n",
        "    train_data = train_data_gen.flow_from_directory(\n",
        "            path,\n",
        "            target_size=image_size,\n",
        "            color_mode=\"rgb\",\n",
        "            batch_size=batch_size,\n",
        "            class_mode=\"sparse\",\n",
        "            shuffle=True,\n",
        "            seed = 0,\n",
        "            subset=\"training\"\n",
        "        )\n",
        "        \n",
        "    val_data = train_data_gen.flow_from_directory(\n",
        "        path,\n",
        "        target_size=image_size,\n",
        "        color_mode=\"rgb\",\n",
        "        batch_size=batch_size,\n",
        "        class_mode=\"sparse\",\n",
        "        shuffle=True,\n",
        "        seed=0,\n",
        "        subset=\"validation\"\n",
        "    )\n",
        "\n",
        "    #Gets the list of class labels.\n",
        "    class_labels = list(train_data.class_indices.keys())\n",
        "\n",
        "\n",
        "    return train_data, val_data, class_labels\n",
        "\n",
        "# Generating the test dataset.\n",
        "def generate_batch_test(path, batch_size, image_size):\n",
        "    #Generates batches of test data.\n",
        "    test_data_gen = ImageDataGenerator(\n",
        "    rescale = 1./255\n",
        "    )\n",
        "\n",
        "    test_data = test_data_gen.flow_from_directory(\n",
        "            path,\n",
        "            target_size=image_size,\n",
        "            color_mode=\"rgb\",\n",
        "            batch_size=batch_size,\n",
        "            class_mode=\"sparse\",\n",
        "            shuffle=True,\n",
        "            seed=0,\n",
        "        )\n",
        "\n",
        "    return test_data\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"
      ],
      "metadata": {
        "id": "9k_bRkHJWD-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training, predicting and logging the experiments. \n",
        "def train(config = None ):\n",
        "\n",
        "    # Commenting the wandb tracking.\n",
        "    # wandb.init(config = config)\n",
        "    # config = wandb.config\n",
        "    \n",
        "    # Reading and setting the configuation\n",
        "    batch_size = config['batch_size']\n",
        "    augmentation = config['augmentation']\n",
        "    pretrain_model = config['pretrain_model']\n",
        "    droprate = config['droprate']\n",
        "    batch_norm = config['batch_normalization']\n",
        "    epoch = config['epoch']\n",
        "    fc_size = config[\"fc_size\"]\n",
        "    num_of_trainable_layers = config['num_of_trainable_layers']\n",
        "\n",
        "    # wandb.run.name =pretrain_model+'_'+'_bs_'+str(batch_size)+'_'+'_i_'+str(epoch)+'_aug_'+str('_aug_' if augmentation else '')+'_fc_size_'+str(fc_size)+str('_bn_' if batch_norm else '')+'_dr_'+str(droprate)+\"trainable_layers\"+str(num_of_trainable_layers)\n",
        "\n",
        "    # Choosing the pretrained model based on configuration input.\n",
        "    if pretrain_model == 'InceptionV3':\n",
        "        image_size = (299,299)\n",
        "        base_model = tf.keras.applications.InceptionV3(include_top = False,weights='imagenet', input_shape=image_size+(3,))\n",
        "\n",
        "    elif pretrain_model == 'InceptionResNetV2':\n",
        "        image_size = (299,299)\n",
        "        base_model = tf.keras.applications.InceptionResNetV2(include_top = False,weights='imagenet',input_shape=image_size+(3,))\n",
        "\n",
        "    elif pretrain_model == 'ResNet50':\n",
        "        image_size = (224,224)\n",
        "        base_model = tf.keras.applications.ResNet50(include_top = False,weights='imagenet',input_shape=image_size+(3,))\n",
        "    \n",
        "    elif pretrain_model == 'Xception':\n",
        "        image_size = (299,299)\n",
        "        base_model = tf.keras.applications.Xception(include_top = False,weights='imagenet',input_shape=image_size+(3,))\n",
        "    \n",
        "    elif pretrain_model == 'MobileNetV2':\n",
        "        image_size = (224,224)\n",
        "        base_model = tf.keras.applications.MobileNetV2(include_top = False,weights='imagenet',input_shape=image_size+(3,))\n",
        "\n",
        "    # Freezing the pretrained model's layer. \n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Adding the new fully connected layer on top of the feature extraction layers of pretrained model.\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=image_size+(3,)),\n",
        "        base_model,\n",
        "        Flatten(),\n",
        "        Dense(fc_size,activation='relu'),\n",
        "    ])\n",
        "\n",
        "    if batch_norm:\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Dropout(droprate))\n",
        "    model.add(Dense(fc_size, activation='relu'))\n",
        "    model.add(Dropout(droprate))\n",
        "    train_data,val_data,class_labels = generate_batch_train_val(trainset_dir, augmentation, batch_size,image_size)\n",
        "    model.add(Dense(len(class_labels) ,activation='softmax'))   \n",
        "\n",
        "    # Setting the optimization and loss function.\n",
        "    model.compile(\n",
        "        optimizer= 'adam',\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Dividing the epoch between pretraining and fine-tuning(if asked).\n",
        "    if num_of_trainable_layers > 0:\n",
        "        fine_tuning_epoch = int(epoch/2)\n",
        "        pretrain_epoch = int(epoch/2)\n",
        "    else:\n",
        "        pretrain_epoch = epoch\n",
        "    \n",
        "    #Setting the wandb callback function.\n",
        "    # wandb_callback = wandb.keras.WandbCallback(monitor=\"val_accuracy\")\n",
        "    \n",
        "    # Training the model.\n",
        "    hist=model.fit(train_data,epochs=pretrain_epoch,validation_data=val_data)#,callbacks=[wandb_callback]\n",
        "    \n",
        "\n",
        "    # Fine-tuning\n",
        "    # Based on input, if number of trainable layers are >0, then setting that number of the freezed layers in pretrained model trainable.\n",
        "    if num_of_trainable_layers > 0:\n",
        "        num_of_trainable_layers=num_of_trainable_layers+(len(model.layers)-len(base_model.layers))\n",
        "        for layer in reversed(model.layers):\n",
        "            if(num_of_trainable_layers> 0):\n",
        "                layer.trainable=True\n",
        "                num_of_trainable_layers -= 1\n",
        "        \n",
        "        model.compile(\n",
        "            optimizer= tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "            metrics=['accuracy'])\n",
        "        \n",
        "        # Fine tuning.\n",
        "        hist=model.fit(train_data,epochs=fine_tuning_epoch,validation_data=val_data)#,callbacks=[wandb_callback])\n",
        "\n",
        "\n",
        "#Reference : Tensorflow Documentation."
      ],
      "metadata": {
        "id": "79QNft-wlTmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tO5yfNcxfryG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best parameters configuration run.\n",
        "config = {\n",
        "    'pretrain_model': 'Xception',\n",
        "    'epoch':9,\n",
        "    'batch_size': 16,\n",
        "    'augmentation': True,\n",
        "    'fc_size': 256,\n",
        "    'droprate':0.4,\n",
        "    'batch_normalization': True,\n",
        "    'num_of_trainable_layers' : 1\n",
        "    }\n",
        "train(config)"
      ],
      "metadata": {
        "id": "gQKLSOUxqcvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sweep configuration\n",
        "sweep_config = {\n",
        "    'name': 'A2_B_bayes',\n",
        "    'method': 'bayes',\n",
        "    'early_terminate':{'type': 'hyperband', 'min_iter': 3},\n",
        "    'metric':{'name':'val_Accuracy','goal':'maximize'},\n",
        "    'parameters': {\n",
        "        'pretrain_model' : {'values' :['InceptionV3','InceptionResNetV2','ResNet50','Xception','MobileNetV2']},\n",
        "        'epoch' : {'values':[6,9]},\n",
        "        'batch_size' : {'values':[16,32,128]},\n",
        "        'augmentation':{'values':[True,False]},\n",
        "        'fc_size': {'values': [128,256,512]},\n",
        "        'droprate':{'values': [0.4,0.5]},\n",
        "        'batch_normalization': {'values':[True,False]},\n",
        "        'num_of_trainable_layers' : {'values': [0,1,2]},\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "# Running for a particular sweep id.\n",
        "# sweep_id = '8hfvwkv6' # wandb.sweep( sweep_config,project='CS6910_A2',entity='cs6910_a2')\n",
        "# wandb.agent(sweep_id, function=train)#count = 10"
      ],
      "metadata": {
        "id": "-scQPcOkYJ0Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}